def utils, streams, s3_bucket
node {
    checkout scm
    utils = load("utils.groovy")
    streams = load("streams.groovy")
    pod = readFile(file: "manifests/pod.yaml")

    s3_bucket = utils.get_pipeline_annotation('s3-bucket')
}

properties([
    pipelineTriggers([]),
    parameters([
      choice(name: 'STREAM',
             // list devel first so that it's the default choice
             choices: (streams.development + streams.production + streams.mechanical),
             description: 'Fedora CoreOS stream to release'),
      string(name: 'VERSION',
             description: 'Fedora CoreOS version to release',
             defaultValue: '',
             trim: true),
      // Default to true for AWS_REPLICATION because the only case
      // where we are running the job by hand is when we're doing a
      // production release and we want to replicate there. Defaulting
      // to true means there is less opportunity for human error.
      //
      // use a string here because passing booleans via `oc start-build -e`
      // is non-trivial
      choice(name: 'AWS_REPLICATION',
             choices: (['true', 'false']),
             description: 'Force AWS AMI replication'),
      string(name: 'COREOS_ASSEMBLER_IMAGE',
             description: 'Override coreos-assembler image to use',
             defaultValue: "coreos-assembler:master",
             trim: true)
    ])
])

currentBuild.description = "[${params.STREAM}] - ${params.VERSION}"

// substitute the right COSA image into the pod definition before spawning it
pod = pod.replace("COREOS_ASSEMBLER_IMAGE", params.COREOS_ASSEMBLER_IMAGE)

// shouldn't need more than 256Mi for this job
pod = pod.replace("COREOS_ASSEMBLER_MEMORY_REQUEST", "256Mi")

echo "Final podspec: ${pod}"

// use a unique label to force Kubernetes to provision a separate pod per run
def pod_label = "cosa-${UUID.randomUUID().toString()}"

// We just lock here out of an abundance of caution in case somehow two release
// jobs run for the same stream, but that really shouldn't happen. Anyway, if it
// *does*, this makes sure they're run serially.
lock(resource: "release-${params.STREAM}") {
podTemplate(cloud: 'openshift', label: pod_label, yaml: pod) {
    node(pod_label) { container('coreos-assembler') {

        def s3_stream_dir = "${s3_bucket}/prod/streams/${params.STREAM}"
        def gcp_image = ""

        // Clone the automation repo, which contains helper scripts. In the
        // future, we'll probably want this either part of the cosa image, or
        // in a derivative of cosa for pipeline needs.
        utils.shwrap("""
        git clone --depth=1 https://github.com/coreos/fedora-coreos-releng-automation /var/tmp/fcos-releng
        """)

        // Fetch metadata for the build we are interested in
        stage('Fetch Metadata') {
            utils.shwrap("""
            export AWS_CONFIG_FILE=\${AWS_FCOS_BUILDS_BOT_CONFIG}
            cosa init --branch ${params.STREAM} https://github.com/coreos/fedora-coreos-config
            cosa buildprep --build=${params.VERSION} s3://${s3_stream_dir}/builds
            """)

            def basearch = utils.shwrap_capture("cosa basearch")
            meta_json = "builds/${params.VERSION}/${basearch}/meta.json"
            def meta = readJSON file: meta_json
            if (meta.gcp.image) {
                gcp_image = meta.gcp.image
            }
        }

        // For production streams, import the OSTree into the prod
        // OSTree repo.
        if ((params.STREAM in streams.production) && utils.path_exists("/etc/fedora-messaging-cfg/fedmsg.toml")) {
            stage("OSTree Import: Prod Repo") {
                utils.shwrap("""
                /var/tmp/fcos-releng/coreos-ostree-importer/send-ostree-import-request.py \
                    --build=${params.VERSION} --s3=${s3_stream_dir} --repo=prod \
                    --fedmsg-conf=/etc/fedora-messaging-cfg/fedmsg.toml
                """)
            }
        }

        // For production streams, promote the GCP image so that it
        // will be the chosen image in an image family and deprecate
        // all others. `ore gcloud promote-image` does this for us.
        if (params.STREAM in streams.production) {
            stage('GCP: Image Promotion') {
                utils.shwrap("""
                # pick up the project to use from the config
                gcp_project=\$(jq -r .project_id \${GCP_IMAGE_UPLOAD_CONFIG})
                ore gcloud promote-image \
                    --log-level=INFO \
                    --project=\${gcp_project} \
                    --json-key \${GCP_IMAGE_UPLOAD_CONFIG} \
                    --family fedora-coreos-${params.STREAM} \
                    --image "${gcp_image}"
                """)
            }
        }

        if (params.AWS_REPLICATION == 'true') {
            // Replicate the newly uploaded AMI to other regions. Intentionally
            // split out from the 'Upload AWS' stage to allow for tests to be added
            // at a later date before replicating said image.
            //
            // We have to re-run the coreos-meta-translator as aws-replicate
            // only modifies the meta.json
            stage('Replicate AWS AMI') {
                utils.shwrap("""
                export AWS_CONFIG_FILE=\${AWS_FCOS_BUILDS_BOT_CONFIG}
                cosa aws-replicate --build=${params.VERSION} --log-level=INFO
                /var/tmp/fcos-releng/coreos-meta-translator/trans.py --build-id ${params.VERSION} --workdir .
                cosa buildupload --build=${params.VERSION} --skip-builds-json s3 --acl=public-read ${s3_stream_dir}/builds
                """)
            }
        }

        stage('Publish') {
          // Run plume to publish official builds; This will handle modifying
          // object ACLs, modifying AMI image attributes,
          // and creating/modifying the releases.json metadata index
          utils.shwrap("""
          export AWS_CONFIG_FILE=\${AWS_FCOS_BUILDS_BOT_CONFIG}
          plume release --distro fcos \
              --version ${params.VERSION} \
              --stream ${params.STREAM} \
              --bucket ${s3_bucket}
          """)
        }
    }}
}}
